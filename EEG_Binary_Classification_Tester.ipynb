{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e02df7",
   "metadata": {},
   "source": [
    "# Testing Notebook\n",
    "\n",
    "This notebook is to input a model from training and test on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad02c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Data management\n",
    "import pandas as pd\n",
    "\n",
    "# Data processing\n",
    "from ssqueezepy import ssq_cwt\n",
    "\n",
    "# Model training\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, TimeDistributed\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f2da8b",
   "metadata": {},
   "source": [
    "## Test Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf917f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test EEG data from dat directory\n",
    "test_file = pd.read_csv('t1_a6_dataset/GO/dk1_test_1673473168.csv', header=None)\n",
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b26778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try test file\n",
    "\n",
    "# Convert to numpy arrays\n",
    "timestamps, tp9, af7, af8, tp10 = test_file.T.to_numpy()\n",
    "\n",
    "# Get the frequency of the samples \n",
    "start = datetime.strptime(str(timestamps[0]), \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "end = datetime.strptime(str(timestamps[-1]), \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "duration = (end-start).total_seconds()\n",
    "fs = timestamps.size/duration\n",
    "\n",
    "Twtp9, Wtp9, *_ = ssq_cwt(tp9, fs=fs)\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "fig.add_subplot(3, 1, 2)\n",
    "plt.imshow(np.abs(Twtp9), aspect='auto', cmap='turbo')\n",
    "fig.add_subplot(3, 1, 1)\n",
    "plt.imshow(np.abs(Wtp9), aspect='auto', cmap='turbo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e6f6a",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280d63a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "10\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Pull all training files data\n",
    "\n",
    "# ENTER FILE EXTS HERE\n",
    "data_path = './datasets/robustness_dataset/'\n",
    "test_name = 'ridvan_test_cont'\n",
    "\n",
    "files = defaultdict(list)\n",
    "file_types = (\"GO\", \"REST\", \"STOP\")\n",
    "# file_types = (\"GO\", \"REST\", \"STOP\", \"REMOVED\")\n",
    "    \n",
    "for file_type in file_types:\n",
    "    \n",
    "    fileglob = os.path.join(data_path, file_type, '%s_1*.csv' % test_name)\n",
    "    \n",
    "    for i, file in enumerate(shuffle(glob.glob(fileglob))):\n",
    "        files[file_type].append(pd.read_csv(file, header=None))\n",
    "\n",
    "    print(len(files[file_type]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb608ff0",
   "metadata": {},
   "source": [
    "## Signal Squeeze Wavelet Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07ddf7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_to_ssq(timeseries):\n",
    "    \"\"\"\n",
    "    Converts 4 channel time series data to ssq wavelet transform\n",
    "    \"\"\"\n",
    "    timestamps, tp9, af7, af8, tp10 = timeseries\n",
    "    \n",
    "    # Get the frequency of the samples \n",
    "    start = datetime.strptime(str(timestamps[0]), \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    end = datetime.strptime(str(timestamps[-1]), \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    duration = (end-start).total_seconds()\n",
    "    fs = timestamps.size/duration\n",
    "    \n",
    "    Twtp9, _, *_ = ssq_cwt(tp9, fs=fs)\n",
    "    Twtp10, _, *_ = ssq_cwt(tp10, fs=fs)\n",
    "    Twaf7, _, *_ = ssq_cwt(af7, fs=fs)\n",
    "    Twaf8, _, *_ = ssq_cwt(af8, fs=fs)\n",
    "    \n",
    "    return np.array((Twtp9, Twaf7, Twaf8, Twtp10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b7c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all files into test data\n",
    "X = []\n",
    "t = []\n",
    "\n",
    "key_dict = {\"STOP\":(0.0, 1.0, 0.0),\n",
    "            \"GO\":(1.0, 0.0, 0.0),\n",
    "            \"REST\": (0.0, 0.0, 1.0)}\n",
    "\n",
    "original_sample_size = 3000\n",
    "buffer_sample_size = 500\n",
    "training_sample_size = 25\n",
    "\n",
    "def compile_files(files, file_type):\n",
    "    X = []\n",
    "    t = []\n",
    "    \n",
    "    num_files = sum([len(x) for x in files.values()])\n",
    "    \n",
    "    for key, data in files.items():\n",
    "        for recording in data:\n",
    "            # if file is smaller than 3000 samples, don't use it\n",
    "            if len(recording[0]) < original_sample_size:\n",
    "                continue\n",
    "\n",
    "            # Split file into buffer size samples\n",
    "            time_samples = np.split(recording.head(original_sample_size).T.to_numpy(),\n",
    "                                         int(original_sample_size/buffer_sample_size), axis=1)\n",
    "\n",
    "            for time_sample in time_samples:\n",
    "                # Convert to numpy arrays\n",
    "                ssq_data = ts_to_ssq(time_sample)\n",
    "\n",
    "                # Take the magnitude of the wavelet transform only\n",
    "                ssq_data = np.abs(ssq_data)\n",
    "\n",
    "                # Must transpose data to (frame, height, channel) for training\n",
    "                ssq_data = ssq_data.transpose(2,1,0)\n",
    "\n",
    "                # Split sample into many blocks\n",
    "                wavelet_samples = np.split(ssq_data, int(buffer_sample_size/training_sample_size))\n",
    "\n",
    "                X.extend(wavelet_samples)\n",
    "                t.extend([key_dict[key] for i in range(len(wavelet_samples))])\n",
    "                \n",
    "            num_files -= 1\n",
    "            print(\"%d %s files left to convert \" % (num_files, file_type), end=\"\\r\")\n",
    "                \n",
    "    return np.array(X), np.array(t)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd93b211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 val files left to convert  \r"
     ]
    }
   ],
   "source": [
    "X, t = compile_files(files, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "214a6150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 (25, 229, 4)\n",
      "2400 [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(len(X), X[0].shape)\n",
    "print(len(t), t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7071398",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "X, t = shuffle(np.array(X), np.array(t), random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f5118",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98b363a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/trial2/attempt1_500.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea4d2811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 8s 69ms/step - loss: 1.8428 - accuracy: 0.5967\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X, t, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "905f4515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.8051917e-02, 9.7133261e-01, 6.1546796e-04],\n",
       "       [1.7536689e-04, 2.8033589e-03, 9.9702126e-01],\n",
       "       [1.1013484e-08, 1.0810739e-06, 9.9999893e-01],\n",
       "       [5.5851316e-01, 4.4096178e-01, 5.2503520e-04],\n",
       "       [1.8192174e-04, 2.0645137e-03, 9.9775356e-01],\n",
       "       [1.6177658e-02, 6.0884252e-02, 9.2293805e-01],\n",
       "       [4.0439241e-02, 9.5952600e-01, 3.4817276e-05],\n",
       "       [1.7591206e-06, 1.5782272e-04, 9.9984038e-01],\n",
       "       [3.1723025e-01, 6.4368618e-01, 3.9083496e-02],\n",
       "       [4.6396524e-02, 9.5359504e-01, 8.3603909e-06]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "567e5f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce7cac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
