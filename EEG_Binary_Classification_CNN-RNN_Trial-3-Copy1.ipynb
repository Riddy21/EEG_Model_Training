{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31de1cd",
   "metadata": {},
   "source": [
    "# EEG Wheelchair Binary Classification ML Model\n",
    "## Using 1D Convolutional Networks and Recurrent Networks\n",
    "This is the first experiment with trying to convert 4 channel EEG brain wave data into a binary classification of stop and go for the EEG wheelchair control system.\n",
    "\n",
    "## Overview\n",
    "The following notebook will follow these next steps:\n",
    "1. Data Cleaning\n",
    "    - Get rid of inconsistent samples or bad samples\n",
    "2. Data Preprocessing\n",
    "    - Convert to wavelet transforms and take signal squeezed signals\n",
    "3. Data Filtering\n",
    "    - Filter out irrelevant frequencies and disconnections\n",
    "4. Build Model\n",
    "    - Form CNN-RNN network for prediction modelling\n",
    "5. Model Training\n",
    "    - Train model on training set of EEG Samples\n",
    "6. Hyperparameter Optimization\n",
    "    - Optimize model hyperparameters by cross validation \n",
    "7. Model Validation\n",
    "    - Validate model on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e41fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import random\n",
    "\n",
    "# Data management\n",
    "import pandas as pd\n",
    "\n",
    "# Data processing\n",
    "from ssqueezepy import ssq_cwt\n",
    "\n",
    "# Model training\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, TimeDistributed, Concatenate, Reshape\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, AveragePooling1D\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5880c54",
   "metadata": {},
   "source": [
    "## Experiment Information\n",
    "Record the experiment information in these fields to be saved with the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805987aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_num = 3\n",
    "attempt_num = 9\n",
    "input_dim = 195\n",
    "output_dim = 3\n",
    "attempt_description = \\\n",
    "\"\"\"\n",
    "## 9th attempt - Multi channel Test\n",
    "\n",
    "### Changes made\n",
    "- Trying the model with different dataset found online\n",
    "    - https://www.nature.com/articles/sdata2018211\n",
    "- Using the LR hand paradigm\n",
    "- Data inforamtion:\n",
    "    - Original model, 34 samples per prediction\n",
    "    - 4 Channels\n",
    "- Changing the input data shape\n",
    "    - from (frame, sample, height, channel)\n",
    "    - to (frame, channel, sample, height)\n",
    "- Building a new model\n",
    "    - Seperating model into 4 different channels before running\n",
    "- Experimenting different CNN architectures to find the optimal\n",
    "    - Trying to add more layers to the CNN\n",
    "    - Changing CNN sizing to see effect\n",
    "    - Changing CNN stride size and kernel size and see effect\n",
    "- GRU Recurrent network\n",
    "- learning rate of 0.01\n",
    "\n",
    "### Expected results\n",
    "- See if we can get more validation and training data correlation\n",
    "\n",
    "### Issues noticed\n",
    "- Looks like training could not pass 50% accuracy\n",
    "    - likely means that model with multiple channels could not understand the data\n",
    "- Increasing the sample size seems to improve the accuracy by can't do more than that\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878854c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories to save data\n",
    "model_dir = './models/trial%s' % trial_num\n",
    "results_dir = './results/trial%s/attempt%s' % (trial_num, attempt_num)\n",
    "interm_dir = './processed_data/trial%s/attempt%s' % (trial_num, attempt_num)\n",
    "\n",
    "model_file = os.path.join(model_dir, 'attempt%s_%s.h5' % (attempt_num, input_dim))\n",
    "description_file = os.path.join(results_dir, 'attempt%s_readme.md' % attempt_num)\n",
    "accuracy_graph = os.path.join(results_dir, 'attempt%s_accuracy.png' % attempt_num)\n",
    "loss_graph = os.path.join(results_dir, 'attempt%s_loss.png' % attempt_num)\n",
    "conf_matrix = os.path.join(results_dir, 'attempt%s_confusion.png' % attempt_num)\n",
    "log_file = os.path.join(results_dir, 'attempt%s.log' % attempt_num)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "if not os.path.exists(interm_dir):\n",
    "    os.makedirs(interm_dir)\n",
    "    \n",
    "# Save description\n",
    "with open(description_file, 'w') as f:\n",
    "    f.write(attempt_description)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb0e2b",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FOR RUNNING OPENBCI DATA ####\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('./datasets/openbci_dataset/CLA-SubjectJ-170508-3St-LRHand-Inter.mat')\n",
    "mat2 = scipy.io.loadmat('./datasets/openbci_dataset/CLA-SubjectJ-170510-3St-LRHand-Inter.mat')\n",
    "data = np.concatenate((mat['o']['data'][0][0], mat2['o']['data'][0][0]))\n",
    "labels = np.concatenate((mat['o']['marker'][0][0].flatten(),mat2['o']['marker'][0][0].flatten()))\n",
    "print(labels.shape, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d2b86",
   "metadata": {},
   "source": [
    "### Try test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b455e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try test file\n",
    "\n",
    "# Convert to numpy arrays\n",
    "tp9, af7, af8, tp10 = data[621892-1000:621892+1000].T[:4]\n",
    "\n",
    "# Get the frequency of the samples \n",
    "Twtp9, Wtp9, *_ = ssq_cwt(tp9, fs=200)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "fig.add_subplot(4, 1, 1)\n",
    "plt.plot(data[621892-1000:621892+1000])\n",
    "fig.add_subplot(4, 1, 3)\n",
    "plt.imshow(np.abs(Twtp9), aspect='auto', cmap='turbo')\n",
    "fig.add_subplot(4, 1, 2)\n",
    "plt.imshow(np.abs(Wtp9), aspect='auto', cmap='turbo')\n",
    "fig.add_subplot(4, 1, 4)\n",
    "plt.plot(labels[621892-1000:621892+1000])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0048c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### For our data\n",
    "segments = defaultdict(list)\n",
    "\n",
    "# Number of samples per file\n",
    "SEGMENT_SIZE = 170\n",
    "\n",
    "# Number of channels per file\n",
    "CHANNELS = [1, 2, 17, 18]\n",
    "# CHANNELS = [i for i in range(1,23)]\n",
    "\n",
    "# label_map = {1: \"LEFT\",\n",
    "#              2: \"RIGHT\",\n",
    "#              3: \"PASSIVE\",\n",
    "#              0: \"REMOVED\"}\n",
    "\n",
    "# Only use the left right passive signals\n",
    "label_map = {1: \"LEFT\",\n",
    "             2: \"RIGHT\",\n",
    "             3: \"PASSIVE\"}\n",
    "\n",
    "# Compile each 1s session into file\n",
    "segment = []\n",
    "for i in range(len(labels)):\n",
    "    try:\n",
    "        label = label_map[labels[i]]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    # Append the previous file and open new file\n",
    "    if i+1 == len(labels) or labels[i] != labels[i+1] or len(segment) == SEGMENT_SIZE:\n",
    "        # Split any that are large then FILE_SIZE and discard any that is smaller\n",
    "        if len(segment) == SEGMENT_SIZE:\n",
    "            segments[label].append(np.array(segment).T)\n",
    "        segment = []\n",
    "        \n",
    "    segment.append(np.take(data[i], [chan - 1 for chan in CHANNELS]))\n",
    "    \n",
    "    \n",
    "print(\"Total:\")\n",
    "for val in label_map.values():\n",
    "    print('    There are %d segments with %s label of shape %s' % (len(segments[val]), val, segments[val][0].shape))\n",
    "    \n",
    "max_num_segments = min([len(segments[key]) for key in label_map.values()])\n",
    "\n",
    "train_segments = defaultdict(list)\n",
    "test_segments = defaultdict(list)\n",
    "val_segments = defaultdict(list)\n",
    "\n",
    "#             [train, test, vali]\n",
    "split_ratio = [0.8, 0.1, 0.1]\n",
    "\n",
    "# Split the data to training test and val\n",
    "for key in label_map.values():\n",
    "    train_segments[key], test_segments[key], val_segments[key] = \\\n",
    "            np.split(shuffle(segments[key])[:max_num_segments], # Shuffle the data\n",
    "                     [int(sum(split_ratio[:i+1])*max_num_segments) for i in range(len(split_ratio)-1)])\n",
    "\n",
    "print(\"Training: \")\n",
    "for val in label_map.values():\n",
    "    print('    There are %d segments with %s label of shape %s' % (len(train_segments[val]), val, train_segments[val].shape))\n",
    "    \n",
    "print(\"Testing: \")\n",
    "for val in label_map.values():\n",
    "    print('    There are %d segments with %s label of shape %s' % (len(test_segments[val]), val, test_segments[val].shape))\n",
    "    \n",
    "print(\"Validation: \")\n",
    "for val in label_map.values():\n",
    "    print('    There are %d segments with %s label of shape %s' % (len(val_segments[val]), val, val_segments[val].shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac7e119",
   "metadata": {},
   "source": [
    "## Convert to wavelet transform signal squeezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 200\n",
    "def ts_to_ssq(timeseries):\n",
    "    \"\"\"\n",
    "    Converts x channel time series data to ssq wavelet transform\n",
    "    \"\"\"\n",
    "    ssqcwt = []\n",
    "    for channel in timeseries:\n",
    "        out, _, *_ = ssq_cwt(channel, fs=SAMPLE_RATE)\n",
    "        ssqcwt.append(out)\n",
    "    \n",
    "    return np.array(ssqcwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_frame_size = SEGMENT_SIZE\n",
    "buffer_frame_size = SEGMENT_SIZE # 0.85s\n",
    "training_frame_size = 34\n",
    "# training_sample_size = SEGMENT_SIZE\n",
    "\n",
    "key_dict = {\"LEFT\":(1.0, 0.0, 0.0),\n",
    "            \"RIGHT\":(0.0, 1.0, 0.0),\n",
    "            \"PASSIVE\": (0.0, 0.0, 1.0)}\n",
    "\n",
    "\n",
    "def compile_segments(segments, seg_type):\n",
    "    X = []\n",
    "    t = []\n",
    "    \n",
    "    num_segments = sum([len(x) for x in segments.values()])\n",
    "    \n",
    "    for key, data in segments.items():\n",
    "        for recording in data:\n",
    "            # if segment is smaller than sample size\n",
    "            if len(recording[0]) < original_frame_size:\n",
    "                continue\n",
    "\n",
    "            # Split file into buffer size samples\n",
    "            time_samples = np.split(recording,\n",
    "                                    int(original_frame_size/buffer_frame_size), axis=1)\n",
    "\n",
    "            for time_sample in time_samples:\n",
    "                # Convert to numpy arrays\n",
    "                ssq_data = ts_to_ssq(time_sample)\n",
    "\n",
    "                # Take the magnitude of the wavelet transform only\n",
    "                ssq_data = np.abs(ssq_data)\n",
    "\n",
    "                # Split sample into many blocks\n",
    "                wavelet_samples = np.split(ssq_data, int(buffer_frame_size/training_frame_size))\n",
    "\n",
    "                X.extend(wavelet_samples)\n",
    "                t.extend([key_dict[key] for i in range(len(wavelet_samples))])\n",
    "                \n",
    "            num_segments -= 1\n",
    "            \n",
    "            print(\"%d %s segments left to convert \" % (num_segments, seg_type), end=\"\\r\")\n",
    "                \n",
    "    return np.array(X), np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27cea15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile all files into train data\n",
    "X_train, t_train = compile_segments(train_segments, 'train')\n",
    "\n",
    "X_test, t_test = compile_segments(test_segments, 'test')\n",
    "\n",
    "X_val, t_val = compile_segments(val_segments, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f695b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Training data\")\n",
    "print(X_train.shape)\n",
    "print(t_train.shape)\n",
    "print(\"Testing data\")\n",
    "print(X_test.shape)\n",
    "print(t_test.shape)\n",
    "print(\"Validation data\")\n",
    "print(X_val.shape)\n",
    "print(t_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef006cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ssq data\n",
    "np.save(os.path.join(interm_dir, 'x_train.npy'), X_train)\n",
    "np.save(os.path.join(interm_dir, 't_train.npy'), t_train)\n",
    "np.save(os.path.join(interm_dir, 'x_test.npy'), X_test)\n",
    "np.save(os.path.join(interm_dir, 't_test.npy'), t_test)\n",
    "np.save(os.path.join(interm_dir, 'x_val.npy'), X_val)\n",
    "np.save(os.path.join(interm_dir, 't_val.npy'), t_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a794b1a",
   "metadata": {},
   "source": [
    "## *Shortcut: Load Preprocessed Saved Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7799e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ssq data\n",
    "interm_dir = './processed_data/trial3/attempt7/'\n",
    "X_train = np.load(os.path.join(interm_dir, 'x_train.npy'))\n",
    "t_train = np.load(os.path.join(interm_dir, 't_train.npy'))\n",
    "X_test = np.load(os.path.join(interm_dir, 'x_test.npy'))\n",
    "t_test = np.load(os.path.join(interm_dir, 't_test.npy'))\n",
    "X_val = np.load(os.path.join(interm_dir, 'x_val.npy'))\n",
    "t_val = np.load(os.path.join(interm_dir, 't_val.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b83e2",
   "metadata": {},
   "source": [
    "## Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a39d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "X_train, t_train = shuffle(X_train, t_train)\n",
    "X_test, t_test = shuffle(X_test, t_test)\n",
    "X_val, t_val = shuffle(X_val, t_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1551114",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train\n",
    "X_train = X_train[:1000]\n",
    "t_train_full = t_train\n",
    "t_train = t_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb1be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = to_categorical(X_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e5c6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for training\n",
    "X_train = X_train.transpose(1, 0, 2, 3)\n",
    "\n",
    "X_test = X_test.transpose(1, 0, 2, 3)\n",
    "\n",
    "# Reshape the data for training\n",
    "X_val = X_val.transpose(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76cc08ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 6795, 195, 34)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ffeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [0, 0, 0]\n",
    "for data in t_train:\n",
    "    for i in range(3):\n",
    "        if data[i] == 1:\n",
    "            counts[i] += 1\n",
    "  \n",
    "print(\"Balance of categories\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645adbf",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14cb772c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "num_frames = len(X_train[0])\n",
    "num_channels = len(X_train)\n",
    "frame_size = len(X_train[0][0])\n",
    "sample_height = len(X_train[0][0][0])\n",
    "\n",
    "inputs = []\n",
    "cnn_out = []\n",
    "\n",
    "print('Training data dimensions = (%s, %s, %s, %s)' % (num_frames, num_channels, frame_size, sample_height))\n",
    "\n",
    "for channel_num in range(num_channels):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(frame_size, sample_height, 1))\n",
    "\n",
    "    # 1st Conv\n",
    "    conv1d = TimeDistributed(\n",
    "                Conv1D(filters=256, kernel_size=(5), strides=1, padding='same', activation='relu'))(input_layer)\n",
    "    conv1d = TimeDistributed(\n",
    "                Conv1D(filters=256, kernel_size=(5), strides=1, padding='same', activation='relu'))(conv1d)\n",
    "    \n",
    "        # Pooling layer\n",
    "    pool = TimeDistributed(\n",
    "                MaxPooling1D(pool_size=(2), strides=2))(conv1d)\n",
    "    \n",
    "    conv1d = TimeDistributed(\n",
    "                Conv1D(filters=64, kernel_size=(3), strides=1, padding='same', activation='relu'))(pool)\n",
    "    \n",
    "        # Pooling layer\n",
    "    pool = TimeDistributed(\n",
    "                MaxPooling1D(pool_size=(2), strides=2))(conv1d)\n",
    "    \n",
    "    flatten = TimeDistributed(Flatten())(pool)\n",
    "    \n",
    "    cnn_out.append(flatten)\n",
    "    inputs.append(input_layer)\n",
    "    \n",
    "# Concatenate all channels\n",
    "concat = Concatenate()(cnn_out)\n",
    "\n",
    "# Get recurrent network for time distributed\\\n",
    "lstm = GRU(128, activation='relu', return_sequences=False)(concat)\n",
    "\n",
    "# FIXME: This part is too big\n",
    "# Get flattened of time distributed\n",
    "# time_flat = Flatten()(concat)\n",
    "# # # Do convolution on time flattened\n",
    "# reshape = Reshape(time_flat.shape[1:] + (1,))(time_flat)\n",
    "# time_conv1d = Conv1D(filters=64*num_channels, kernel_size=(5))(reshape)\n",
    "\n",
    "# concat = Concatenate()([Flatten()(time_conv1d), lstm])\n",
    "\n",
    "# final interpretation layers\n",
    "drop = Dropout(0.2)(lstm)\n",
    "dense = Dense(128, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')(drop)\n",
    "dense = Dense(64, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')(dense)\n",
    "drop = Dropout(0.5)(dense)\n",
    "dense = Dense(48, activation='relu', kernel_regularizer='l2', bias_regularizer='l2')(drop)\n",
    "drop = Dropout(0.4)(dense)\n",
    "\n",
    "output = Dense(len(t_train[0]), activation='softmax')(drop)\n",
    "    \n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f493cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.01, beta_1=0.99, beta_2=0.9999)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2b141c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 18:01:36.897722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:21:00.0 name: Tesla K20Xm computeCapability: 3.5\n",
      "coreClock: 0.732GHz coreCount: 14 deviceMemorySize: 5.57GiB deviceMemoryBandwidth: 232.46GiB/s\n",
      "2023-02-22 18:01:36.897901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-02-22 18:01:36.897973: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-02-22 18:01:36.898044: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2023-02-22 18:01:36.898103: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2023-02-22 18:01:36.898159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-02-22 18:01:36.898216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-02-22 18:01:36.898274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-02-22 18:01:36.899283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2023-02-22 18:01:36.899361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-02-22 18:01:36.899381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2023-02-22 18:01:36.899396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2023-02-22 18:01:36.900895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5300 MB memory) -> physical GPU (device: 0, name: Tesla K20Xm, pci bus id: 0000:21:00.0, compute capability: 3.5)\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f605c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = stdout\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544ba26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stdout = sys.stdout\n",
    "f = open(log_file, 'w')\n",
    "sys.stdout = f\n",
    "results = model.fit(list(X_train), t_train, validation_data=(list(X_val), t_val), batch_size=34, epochs=10)\n",
    "sys.stdout = stdout\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac6764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the model\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa9522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_results = model.evaluate(list(X_test), t_test, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb4994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(list(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6666c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918cf37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.plot([test_results[0] for i in range(len(results.history['loss']))])\n",
    "plt.legend(('training loss', 'validation loss', 'test loss'))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Trial %s Attempt %s Loss\" % (trial_num, attempt_num))\n",
    "plt.grid()\n",
    "plt.savefig(loss_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bdf3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(results.history['accuracy'])\n",
    "plt.plot(results.history['val_accuracy'])\n",
    "plt.plot([test_results[1] for i in range(len(results.history['accuracy']))])\n",
    "plt.legend(('training accuracy', 'validation accuracy', 'test accuracy'))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Trial %s Attempt %s Accuracy\" % (trial_num, attempt_num))\n",
    "plt.grid()\n",
    "plt.savefig(accuracy_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b369a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(list(X_test))\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_test = np.argmax(t_test, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='pred')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=key_dict.keys())\n",
    "plot = disp.plot()\n",
    "disp.figure_.savefig(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce7cac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
