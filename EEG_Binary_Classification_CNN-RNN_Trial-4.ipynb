{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31de1cd",
   "metadata": {},
   "source": [
    "# EEG Wheelchair Binary Classification ML Model\n",
    "## Using 1D Convolutional Networks and Recurrent Networks\n",
    "This is the first experiment with trying to convert 4 channel EEG brain wave data into a binary classification of stop and go for the EEG wheelchair control system.\n",
    "\n",
    "## Overview\n",
    "The following notebook will follow these next steps:\n",
    "1. Data Cleaning\n",
    "    - Get rid of inconsistent samples or bad samples\n",
    "2. Data Preprocessing\n",
    "    - Convert to wavelet transforms and take signal squeezed signals\n",
    "3. Data Filtering\n",
    "    - Filter out irrelevant frequencies and disconnections\n",
    "4. Build Model\n",
    "    - Form CNN-RNN network for prediction modelling\n",
    "5. Model Training\n",
    "    - Train model on training set of EEG Samples\n",
    "6. Hyperparameter Optimization\n",
    "    - Optimize model hyperparameters by cross validation \n",
    "7. Model Validation\n",
    "    - Validate model on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e41fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "# Data management\n",
    "import pandas as pd\n",
    "\n",
    "# Data processing\n",
    "from ssqueezepy import ssq_cwt\n",
    "\n",
    "# Model training\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, SpatialDropout1D, SpatialDropout2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten, InputSpec, Layer, Concatenate, AveragePooling2D, MaxPooling2D, Reshape, Permute\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, DepthwiseConv2D, LayerNormalization\n",
    "from tensorflow.keras.layers import TimeDistributed, Lambda, AveragePooling1D, Add, Conv1D, Multiply\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5880c54",
   "metadata": {},
   "source": [
    "## Experiment Information\n",
    "Record the experiment information in these fields to be saved with the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805987aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_num = 4\n",
    "attempt_num = 5\n",
    "\n",
    "output_dim = 4\n",
    "attempt_description = \\\n",
    "\"\"\"\n",
    "## 5rd attempt\n",
    "\n",
    "### Changes made\n",
    "- Trying new model new training data from interactive movement dataset\n",
    "- SEGMENT_WINDOW_SIZE = 400 # 2s\n",
    "- ACTIVE_WAIT_PERIOD = 400 # 1s\n",
    "- ACTIVE_HOLD_PERIOD = 100 # 0.25s for dealing with accidental presses\n",
    "- Balanced\n",
    "- 8 training sessions\n",
    "\n",
    "### Expected results\n",
    "- See if it performs any worse with our Muse model or better than recorded data\n",
    "- See if it adapts to taking off the headset\n",
    "\n",
    "### Issues noticed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878854c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories to save data\n",
    "model_dir = './models/trial%s' % trial_num\n",
    "results_dir = './results/trial%s/attempt%s' % (trial_num, attempt_num)\n",
    "interm_dir = './processed_data/trial%s/attempt%s' % (trial_num, attempt_num)\n",
    "\n",
    "model_file = os.path.join(model_dir, 'attempt%s.h5' % (attempt_num))\n",
    "summary_file = os.path.join(results_dir, 'attempt%s_summary.txt' % attempt_num)\n",
    "description_file = os.path.join(results_dir, 'attempt%s_readme.md' % attempt_num)\n",
    "accuracy_graph = os.path.join(results_dir, 'attempt%s_accuracy.png' % attempt_num)\n",
    "loss_graph = os.path.join(results_dir, 'attempt%s_loss.png' % attempt_num)\n",
    "conf_matrix = os.path.join(results_dir, 'attempt%s_confusion.png' % attempt_num)\n",
    "log_file = os.path.join(results_dir, 'attempt%s.log' % attempt_num)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "if not os.path.exists(interm_dir):\n",
    "    os.makedirs(interm_dir)\n",
    "    \n",
    "# Save description\n",
    "with open(description_file, 'w') as f:\n",
    "    f.write(attempt_description)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb0e2b",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ba5641",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: \"BRAKE\",\n",
    "             1: \"ACCEL\",\n",
    "             2: \"LEFT\",\n",
    "             3: \"RIGHT\",\n",
    "             4: \"PASSIVE\",\n",
    "             5: \"DISCONNECTED\",\n",
    "             6: \"AMBIGUOUS\"}\n",
    "\n",
    "SEGMENT_WINDOW_SIZE = 400 # 2s\n",
    "ACTIVE_WAIT_PERIOD = 400 # 1s\n",
    "ACTIVE_HOLD_PERIOD = 100 # 0.25s for dealing with accidental presses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35189e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mode_smoothing(data, window):\n",
    "    output = []\n",
    "    for frame in pd.Series(data).rolling(window):\n",
    "        output.append(mode(frame)[0])\n",
    "        \n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f0b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOOTHING_WINDOW = 100 # Eliminates any bumps under 0.25s\n",
    "def compile_speed_segments(files):\n",
    "    X = []\n",
    "    t = []\n",
    "    for file in files:\n",
    "        session = pd.read_csv(file, header=None).to_numpy()\n",
    "        \n",
    "        count = 0\n",
    "        timestamps = session[:, 0]\n",
    "        speeds = session[:, 1]\n",
    "        data = session[:, 3:]\n",
    "        \n",
    "        plt.plot(timestamps, speeds)\n",
    "        plt.title(\"Accel break label\")\n",
    "        plt.show()\n",
    "        # smooth out labels\n",
    "#         speeds = rolling_mode_smoothing(speeds, SMOOTHING_WINDOW)\n",
    "#         plt.plot(timestamps, speeds)\n",
    "#         plt.show()\n",
    "        \n",
    "        discon_segment = []\n",
    "        \n",
    "        for i in range(len(session)-1):\n",
    "            if timestamps[i+1] - timestamps[i] > 1000000000:\n",
    "                print(\"OSC delay too long\")\n",
    "                \n",
    "            # If the data was ambiguous, then skip\n",
    "            if speeds[i] == 6:\n",
    "                continue\n",
    "                \n",
    "            # If the data is disconnected, collect in disconnected segments'\n",
    "            if speeds[i] == 5:\n",
    "                discon_segment.append(data[i])\n",
    "                \n",
    "            # reaching max disconnected size save as segment\n",
    "            if len(discon_segment) == SEGMENT_WINDOW_SIZE:\n",
    "                X.append(discon_segment)\n",
    "                t.append(np.eye(len(label_map))[int(speeds[i])])\n",
    "                discon_segment = []\n",
    "            \n",
    "            # Clear if reached end of disconnect\n",
    "            if speeds[i] == 5 and speeds[i+1] != 5:\n",
    "                discon_segment = []\n",
    "            \n",
    "            if speeds[i] != speeds[i+1]:\n",
    "                # Observe rising edge as data\n",
    "                if speeds[i+1] in (1, 0):\n",
    "                    if (speeds[i+1:i+ACTIVE_HOLD_PERIOD] == speeds[i+1]).all() and \\\n",
    "                            len(data[i-int(SEGMENT_WINDOW_SIZE/2)+1:i+int(SEGMENT_WINDOW_SIZE/2)+1]) == SEGMENT_WINDOW_SIZE:\n",
    "                        # Save speed segments at the rising edge of every active region\n",
    "                        X.append(data[i-int(SEGMENT_WINDOW_SIZE/2)+1:i+int(SEGMENT_WINDOW_SIZE/2)+1])\n",
    "                        t.append(np.eye(len(label_map))[int(speeds[i+1])])\n",
    "                    \n",
    "                    # if the user is still holding the button 2s after the rising edge, then it is passive data\n",
    "                    if (speeds[i+1:i+1+int(ACTIVE_WAIT_PERIOD)+SEGMENT_WINDOW_SIZE] == speeds[i+1]).all() and \\\n",
    "                            len(data[i+1+int(ACTIVE_WAIT_PERIOD):i+1+int(ACTIVE_WAIT_PERIOD)+SEGMENT_WINDOW_SIZE]) == SEGMENT_WINDOW_SIZE:\n",
    "                        X.append(data[i+1+int(ACTIVE_WAIT_PERIOD):i+1+int(ACTIVE_WAIT_PERIOD)+SEGMENT_WINDOW_SIZE])\n",
    "                        t.append(np.eye(len(label_map))[4])\n",
    "\n",
    "    return np.array(X).astype(np.float32), np.array(t).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6d15164",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compile_direction_segments(files):\n",
    "    X = []\n",
    "    t = []\n",
    "\n",
    "    for file in files:\n",
    "        session = pd.read_csv(file, header=None).to_numpy()\n",
    "        \n",
    "        count = 0\n",
    "        timestamps = session[:, 0]\n",
    "        directions = session[:, 2]\n",
    "        data = session[:, 3:]\n",
    "        \n",
    "        plt.plot(timestamps, directions)\n",
    "        plt.title(\"Left right label\")\n",
    "        plt.show()\n",
    "        #plt.plot(timestamps, rolling_mode_smoothing(directions, SMOOTHING_WINDOW))\n",
    "        #plt.show()\n",
    "        \n",
    "        discon_segment = []\n",
    "        straight_segment = []\n",
    "        \n",
    "        for i in range(len(session)-1):\n",
    "            if timestamps[i+1] - timestamps[i] > 1000000000:\n",
    "                print(\"OSC delay too long\")\n",
    "                \n",
    "            # If the data was ambiguous, then skip\n",
    "            if directions[i] == 6:\n",
    "                continue\n",
    "                \n",
    "            # If the data is disconnected, collect in disconnected segments'\n",
    "            if directions[i] == 5:\n",
    "                discon_segment.append(data[i])\n",
    "                \n",
    "            # reaching max disconnected size save as segment\n",
    "            if len(discon_segment) == SEGMENT_WINDOW_SIZE:\n",
    "                X.append(discon_segment)\n",
    "                t.append(np.eye(len(label_map))[int(directions[i])])\n",
    "                discon_segment = []\n",
    "            \n",
    "            # Clear if reached end of disconnect\n",
    "            if directions[i] == 5 and directions[i+1] != 5:\n",
    "                discon_segment = []\n",
    "                \n",
    "            # If the direction is passive, collect as a segment\n",
    "            if directions[i] == 4:\n",
    "                straight_segment.append(data[i])\n",
    "            \n",
    "            if len(straight_segment) == SEGMENT_WINDOW_SIZE:\n",
    "                X.append(straight_segment)\n",
    "                t.append(np.eye(len(label_map))[int(directions[i])])\n",
    "                straight_segment = []\n",
    "                \n",
    "            if directions[i] == 4 and directions[i+1] != 4:\n",
    "                straight_segment = []\n",
    "            \n",
    "            if directions[i] != directions[i+1]:\n",
    "                # Save turn segments at the rising edge of every active region\n",
    "                if directions[i+1] in (2, 3):\n",
    "                    if (directions[i+1:i+ACTIVE_HOLD_PERIOD] == directions[i+1]).all() and \\\n",
    "                            len(data[i-int(SEGMENT_WINDOW_SIZE/2)+1:i+int(SEGMENT_WINDOW_SIZE/2)+1]) == SEGMENT_WINDOW_SIZE:\n",
    "                        X.append(data[i-int(SEGMENT_WINDOW_SIZE/2)+1:i+int(SEGMENT_WINDOW_SIZE/2)+1])\n",
    "                        t.append(np.eye(len(label_map))[int(directions[i+1])])\n",
    "                \n",
    "                \n",
    "    return np.array(X).astype(np.float32), np.array(t).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9347e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './datasets/movement_dataset_v2/'\n",
    "train_name = 'ridvan_train'\n",
    "test_name = 'ridvan_test'\n",
    "val_name = 'ridvan_val'\n",
    "\n",
    "fileglob = glob.glob(os.path.join(data_path, '%s_1*.csv' % train_name))\n",
    "X_train, t_train = compile_speed_segments(fileglob)\n",
    "\n",
    "fileglob = glob.glob(os.path.join(data_path, '%s_1*.csv' % test_name))\n",
    "X_test, t_test = compile_speed_segments(fileglob)\n",
    "\n",
    "fileglob = glob.glob(os.path.join(data_path, '%s_1*.csv' % val_name))\n",
    "X_val, t_val = compile_speed_segments(fileglob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be972259",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(label_map.values(), t_train.sum(axis=0))\n",
    "plt.xticks(rotation=45, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "X_train, t_train = shuffle(X_train, t_train)\n",
    "X_test, t_test = shuffle(X_test, t_test)\n",
    "X_val, t_val = shuffle(X_val, t_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40563fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [0, 0, 0, 0, 0, 0]\n",
    "for data in t_train:\n",
    "    for i in range(len(counts)):\n",
    "        if data[i] == 1:\n",
    "            counts[i] += 1\n",
    "  \n",
    "print(\"Balance of categories\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31439ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance categories\n",
    "def balance_categories(X, t):\n",
    "    max_per_label = min(t.sum(axis=0)[np.nonzero(t.sum(axis=0))])\n",
    "    counts = [max_per_label for i in range(len(t[0]))]\n",
    "    new_X = []\n",
    "    new_t = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if (np.array(new_t).sum(axis=0) + t[i] <= counts).all():\n",
    "            new_X.append(X[i])\n",
    "            new_t.append(t[i])\n",
    "            \n",
    "    return np.array(new_X), np.array(new_t)\n",
    "              \n",
    "X_train, t_train = balance_categories(X_train, t_train)\n",
    "X_test, t_test = balance_categories(X_test, t_test)\n",
    "X_val, t_val = balance_categories(X_val, t_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb46e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[0])\n",
    "plt.show()\n",
    "print(t_train[0])\n",
    "\n",
    "plt.plot(X_train[1])\n",
    "plt.show()\n",
    "print(t_train[1])\n",
    "\n",
    "plt.plot(X_train[2])\n",
    "plt.show()\n",
    "print(t_train[2])\n",
    "\n",
    "plt.plot(X_train[100])\n",
    "plt.show()\n",
    "print(t_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e4dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for training\n",
    "X_train = X_train.transpose(0, 2, 1)\n",
    "\n",
    "X_test = X_test.transpose(0, 2, 1)\n",
    "\n",
    "# Reshape the data for training\n",
    "X_val = X_val.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f695b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Training data\")\n",
    "print(X_train.shape)\n",
    "print(t_train.shape)\n",
    "print(\"Testing data\")\n",
    "print(X_test.shape)\n",
    "print(t_test.shape)\n",
    "print(\"Validation data\")\n",
    "print(X_val.shape)\n",
    "print(t_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef006cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ssq data\n",
    "np.save(os.path.join(interm_dir, 'x_train.npy'), X_train)\n",
    "np.save(os.path.join(interm_dir, 't_train.npy'), t_train)\n",
    "np.save(os.path.join(interm_dir, 'x_test.npy'), X_test)\n",
    "np.save(os.path.join(interm_dir, 't_test.npy'), t_test)\n",
    "np.save(os.path.join(interm_dir, 'x_val.npy'), X_val)\n",
    "np.save(os.path.join(interm_dir, 't_val.npy'), t_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a794b1a",
   "metadata": {},
   "source": [
    "## *Shortcut: Load Preprocessed Saved Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ssq data\n",
    "X_train = np.load(os.path.join(interm_dir, 'x_train.npy'))\n",
    "t_train = np.load(os.path.join(interm_dir, 't_train.npy'))\n",
    "X_test = np.load(os.path.join(interm_dir, 'x_test.npy'))\n",
    "t_test = np.load(os.path.join(interm_dir, 't_test.npy'))\n",
    "X_val = np.load(os.path.join(interm_dir, 'x_val.npy'))\n",
    "t_val = np.load(os.path.join(interm_dir, 't_val.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b83e2",
   "metadata": {},
   "source": [
    "## Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1551114",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train\n",
    "X_train = X_train[:100]\n",
    "t_train_full = t_train\n",
    "t_train = t_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b7347",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645adbf",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb772c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_frames = len(X_train)\n",
    "num_channels = len(X_train[0])\n",
    "frame_size = len(X_train[0][0])\n",
    "\n",
    "print('Training data dimensions = (%s, %s, %s)' % (num_frames, num_channels, frame_size))\n",
    "\n",
    "n_ff = [2,4,8,16]    # Number of frequency filters for each inception module of EEG-ITNet\n",
    "n_sf = [1,1,1,1]     # Number of spatial filters in each frequency sub-band of EEG-ITNet\n",
    "\n",
    "drop_rate = 0.2\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(num_channels, frame_size, 1))\n",
    "\n",
    "block1 = Conv2D(n_ff[0], (1, 16), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_1')(input_layer)\n",
    "block1 = BatchNormalization()(block1)\n",
    "block1 = DepthwiseConv2D((num_channels, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[0], activation = 'linear', \n",
    "                       depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_1')(block1)\n",
    "block1 = BatchNormalization()(block1)\n",
    "block1 = Activation('elu')(block1)\n",
    "\n",
    "#================================\n",
    "\n",
    "block2 = Conv2D(n_ff[1], (1, 32), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_2')(input_layer)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = DepthwiseConv2D((num_channels, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[1], activation = 'linear', \n",
    "                       depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_2')(block2)\n",
    "block2 = BatchNormalization()(block2)\n",
    "block2 = Activation('elu')(block2)\n",
    "\n",
    "#================================\n",
    "\n",
    "block3 = Conv2D(n_ff[2], (1, 64), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_3')(input_layer)\n",
    "block3 = BatchNormalization()(block3)\n",
    "block3 = DepthwiseConv2D((num_channels, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[2], activation = 'linear', \n",
    "                       depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_3')(block3)\n",
    "block3 = BatchNormalization()(block3)\n",
    "block3 = Activation('elu')(block3)\n",
    "\n",
    "#================================\n",
    "\n",
    "block4 = Conv2D(n_ff[3], (1, 128), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_4')(input_layer)\n",
    "block4 = BatchNormalization()(block4)\n",
    "block4 = DepthwiseConv2D((num_channels, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[3], activation = 'linear', \n",
    "                       depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_4')(block4)\n",
    "block4 = BatchNormalization()(block4)\n",
    "block4 = Activation('elu')(block4)\n",
    "\n",
    "#================================\n",
    "\n",
    "block = Concatenate(axis = -1)([block1, block2, block3, block4]) \n",
    "\n",
    "#================================\n",
    "\n",
    "block = AveragePooling2D((1, 4))(block)\n",
    "block_in = Dropout(drop_rate)(block)\n",
    "\n",
    "#================================\n",
    "\n",
    "paddings = tf.constant([[0,0], [0,0], [3,0], [0,0]])\n",
    "block = tf.pad(block_in, paddings, \"CONSTANT\")\n",
    "block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 1))(block)\n",
    "block = BatchNormalization()(block)\n",
    "block = Activation('elu')(block)\n",
    "block = Dropout(drop_rate)(block)\n",
    "block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 1))(block)\n",
    "block = BatchNormalization()(block)\n",
    "block = Activation('elu')(block)\n",
    "block = Dropout(drop_rate)(block)\n",
    "block_out = Add()([block_in, block])\n",
    "\n",
    "\n",
    "paddings = tf.constant([[0,0], [0,0], [6,0], [0,0]])\n",
    "block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
    "block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 2))(block)\n",
    "block = BatchNormalization()(block)\n",
    "block = Activation('elu')(block)\n",
    "block = Dropout(drop_rate)(block)\n",
    "block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 2))(block)\n",
    "block = BatchNormalization()(block)\n",
    "block = Activation('elu')(block)\n",
    "block = Dropout(drop_rate)(block)\n",
    "block_out = Add()([block_out, block])\n",
    "\n",
    "\n",
    "paddings = tf.constant([[0,0], [0,0], [12,0], [0,0]])\n",
    "block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
    "block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 4))(block)\n",
    "block = BatchNormalization()(block)\n",
    "block = Activation('elu')(block)\n",
    "block = Dropout(drop_rate)(block)\n",
    "block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 4))(block)\n",
    "block = BatchNormalization()(block)\n",
    "block = Activation('elu')(block)\n",
    "block = Dropout(drop_rate)(block)\n",
    "block_out = Add()([block_out, block]) \n",
    "\n",
    "\n",
    "paddings = tf.constant([[0,0], [0,0], [24,0], [0,0]])\n",
    "block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
    "block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 8))(block)\n",
    "block = BatchNormalization()(block)\n",
    "block = Activation('elu')(block)\n",
    "block = Dropout(drop_rate)(block)\n",
    "block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 8))(block)\n",
    "block = BatchNormalization()(block)\n",
    "block = Activation('elu')(block)\n",
    "block = Dropout(drop_rate)(block)\n",
    "block_out = Add()([block_out, block]) \n",
    "\n",
    "#================================\n",
    "\n",
    "block = block_out\n",
    "\n",
    "#================================\n",
    "\n",
    "block = Conv2D(28, (1,1))(block)\n",
    "block = BatchNormalization()(block)\n",
    "block = Activation('elu')(block)\n",
    "block = AveragePooling2D((1,4), data_format='Channels_last')(block) #'Channels_last' As CPU will be used for inference\n",
    "block = Dropout(drop_rate)(block) \n",
    "embedded = Flatten()(block)\n",
    "\n",
    "output = Dense(len(t_train[0]), activation='softmax', kernel_constraint = max_norm(0.2))(embedded)\n",
    "    \n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, clipvalue=0.01)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f605c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(summary_file, 'w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "    fh.close()\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = stdout\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544ba26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stdout = sys.stdout\n",
    "f = open(log_file, 'w')\n",
    "sys.stdout = f\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=model_file,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "try:\n",
    "    results = model.fit(X_train, t_train, validation_data=(X_val, t_val), batch_size=32, epochs=3000, shuffle=True)\n",
    "    sys.stdout = stdout\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    sys.stdout = stdout\n",
    "    f.close()\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = stdout\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac6764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the model\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa9522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_results = model.evaluate(X_val, t_val, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb4994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6666c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918cf37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "#plt.plot([test_results[0] for i in range(len(results.history['loss']))])\n",
    "plt.legend(('training loss', 'validation loss', 'test loss'))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Trial %s Attempt %s Loss\" % (trial_num, attempt_num))\n",
    "plt.grid()\n",
    "plt.savefig(loss_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bdf3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(results.history['accuracy'])\n",
    "plt.plot(results.history['val_accuracy'])\n",
    "plt.plot([test_results[1] for i in range(len(results.history['accuracy']))])\n",
    "plt.legend(('training accuracy', 'validation accuracy', 'test accuracy'))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Trial %s Attempt %s Accuracy\" % (trial_num, attempt_num))\n",
    "plt.grid()\n",
    "plt.savefig(accuracy_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b369a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['BRAKE', 'ACCEL', 'PASSIVE', 'DISCONNECTED']\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_test = np.argmax(t_test, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='pred')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "plot = disp.plot()\n",
    "disp.figure_.savefig(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7cac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc381a2",
   "metadata": {},
   "source": [
    "# Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test[0].shape)\n",
    "plt.imshow(X_test[0], interpolation='nearest', aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3d72f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_filter_1 = Model(inputs=input_layer, outputs=block1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = spectral_filter_1.predict(X_test[:1])\n",
    "print(output.shape)\n",
    "plt.imshow(output[0][0], interpolation='nearest', aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b15fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spectral_filter = Model(inputs=input_layer, outputs=block_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4de25c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = all_spectral_filter.predict(X_test[:1])\n",
    "print(output.shape)\n",
    "plt.imshow(output[0][0], interpolation='nearest', aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb4152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spectral_filter = Model(inputs=input_layer, outputs=block_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e5cd07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = all_spectral_filter.predict(X_test[:1])\n",
    "print(output.shape)\n",
    "plt.imshow(output[0][0], interpolation='nearest', aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dd371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = Model(model.input, model.get_layer('dropout_9').output)\n",
    "\n",
    "rows = 5\n",
    "\n",
    "f, axarr = plt.subplots(5, 5)\n",
    "row_counts = [0 for i in range(rows)]\n",
    "\n",
    "for i in range(100):\n",
    "    output = probe.predict(np.array([X_train[i]]))\n",
    "    if t_test[i][0]:\n",
    "        row = 0\n",
    "    elif t_test[i][1]:\n",
    "        row = 1\n",
    "    elif t_test[i][2]:\n",
    "        row = 2\n",
    "    elif t_test[i][3]:\n",
    "        row = 3\n",
    "    else:\n",
    "        row = 4\n",
    "    \n",
    "    try:\n",
    "        axarr[row, row_counts[row]].imshow(output[0][0], interpolation='nearest', aspect='auto')\n",
    "        axarr[row, row_counts[row]].axis('off')\n",
    "        axarr[row, row_counts[row]].axis('off')\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    row_counts[row] += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
