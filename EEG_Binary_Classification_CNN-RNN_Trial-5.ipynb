{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31de1cd",
   "metadata": {},
   "source": [
    "# EEG Wheelchair Binary Classification ML Model\n",
    "## Using 1D Convolutional Networks and Recurrent Networks\n",
    "This is the first experiment with trying to convert 4 channel EEG brain wave data into a binary classification of stop and go for the EEG wheelchair control system.\n",
    "\n",
    "## Overview\n",
    "The following notebook will follow these next steps:\n",
    "1. Data Cleaning\n",
    "    - Get rid of inconsistent samples or bad samples\n",
    "2. Data Preprocessing\n",
    "    - Convert to wavelet transforms and take signal squeezed signals\n",
    "3. Data Filtering\n",
    "    - Filter out irrelevant frequencies and disconnections\n",
    "4. Build Model\n",
    "    - Form CNN-RNN network for prediction modelling\n",
    "5. Model Training\n",
    "    - Train model on training set of EEG Samples\n",
    "6. Hyperparameter Optimization\n",
    "    - Optimize model hyperparameters by cross validation \n",
    "7. Model Validation\n",
    "    - Validate model on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e41fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "# Data management\n",
    "import pandas as pd\n",
    "\n",
    "# Data processing\n",
    "from ssqueezepy import ssq_cwt\n",
    "\n",
    "# Model training\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, SpatialDropout1D, SpatialDropout2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Flatten, InputSpec, Layer, Concatenate, AveragePooling2D, MaxPooling2D, Reshape, Permute\n",
    "from tensorflow.keras.layers import Conv2D, SeparableConv2D, DepthwiseConv2D, LayerNormalization\n",
    "from tensorflow.keras.layers import TimeDistributed, Lambda, AveragePooling1D, Add, Conv1D, Multiply\n",
    "from tensorflow.keras.constraints import max_norm, unit_norm \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5880c54",
   "metadata": {},
   "source": [
    "## Experiment Information\n",
    "Record the experiment information in these fields to be saved with the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805987aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_NUM = 5\n",
    "EXPER_NUM = 9\n",
    "DATA_PATH = './datasets/movement_dataset_v3/'\n",
    "\n",
    "experiemnt_desc = \\\n",
    "\"\"\"\n",
    "# Optimization for stationary model, trying with learning rate changes and balanced categories\n",
    "\n",
    "## Description\n",
    "- Using model found from: https://github.com/vinayakr99/Muse-MotorImageryClassification\n",
    "- Running with cross validation\n",
    "- Trying with only taking intent before action\n",
    "- Trying with only ridvan's data on stationary data\n",
    "- Trying again with balanced categories\n",
    "- Trying again with learning rate experiment\n",
    "\"\"\"\n",
    "\n",
    "LABEL_MAP = {0: \"BRAKE\",\n",
    "             1: \"ACCEL\",\n",
    "             2: \"LEFT\",\n",
    "             3: \"RIGHT\",\n",
    "             4: \"PASSIVE\",\n",
    "             5: \"DISCONNECTED\",\n",
    "             6: \"AMBIGUOUS\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878854c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories to save data\n",
    "\n",
    "RESULTS_DIR = './results/trial%s/experiment%s' % (TRIAL_NUM, EXPER_NUM)\n",
    "\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)\n",
    "    \n",
    "description_file = os.path.join(RESULTS_DIR, 'experiment%s_readme.md' % EXPER_NUM)\n",
    "\n",
    "# Save description\n",
    "with open(description_file, 'w') as f:\n",
    "    f.write(experiemnt_desc)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb0e2b",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35189e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mode_smoothing(data, window):\n",
    "    output = []\n",
    "    for frame in pd.Series(data).rolling(window):\n",
    "        output.append(mode(frame)[0])\n",
    "        \n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f0b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOOTHING_WINDOW = 100 # Eliminates any bumps under 0.25s\n",
    "def compile_speed_segments(files, window, wait, hold):\n",
    "    X = []\n",
    "    t = []\n",
    "    for file in files:\n",
    "        session = pd.read_csv(file, header=None).to_numpy()\n",
    "        \n",
    "        count = 0\n",
    "        timestamps = session[:, 0]\n",
    "        speeds = session[:, 1]\n",
    "        data = session[:, 3:]\n",
    "        \n",
    "#         plt.plot(timestamps, speeds)\n",
    "#         plt.title(\"Accel break label\")\n",
    "#         plt.show()\n",
    "        # smooth out labels\n",
    "#         speeds = rolling_mode_smoothing(speeds, SMOOTHING_WINDOW)\n",
    "#         plt.plot(timestamps, speeds)\n",
    "#         plt.show()\n",
    "        \n",
    "        discon_segment = []\n",
    "        \n",
    "        for i in range(len(session)-1):\n",
    "            if timestamps[i+1] - timestamps[i] > 1000000000:\n",
    "#                 print(\"OSC delay too long\")\n",
    "                pass\n",
    "                \n",
    "            # If the data was ambiguous, then skip\n",
    "            if speeds[i] == 6:\n",
    "                continue\n",
    "                \n",
    "            # If the data is disconnected, collect in disconnected segments'\n",
    "            if speeds[i] == 5:\n",
    "                discon_segment.append(data[i])\n",
    "                \n",
    "            # reaching max disconnected size save as segment\n",
    "            if len(discon_segment) == window:\n",
    "                X.append(discon_segment)\n",
    "                t.append(np.eye(len(LABEL_MAP))[int(speeds[i])])\n",
    "                discon_segment = []\n",
    "            \n",
    "            # Clear if reached end of disconnect\n",
    "            if speeds[i] == 5 and speeds[i+1] != 5:\n",
    "                discon_segment = []\n",
    "            \n",
    "            if speeds[i] != speeds[i+1]:\n",
    "                # Observe rising edge as data\n",
    "                if speeds[i+1] in (1, 0):\n",
    "                    # If the speed stays constant for wait time\n",
    "                    if (speeds[i+1:i+wait] == speeds[i+1]).all() and \\\n",
    "                            len(data[i-int(window/2)+1:i+int(window/2)+1]) == window:\n",
    "                        # Save speed segments at the rising edge of every active region\n",
    "                        X.append(data[i-int(window/2)+1:i+int(window/2)+1])\n",
    "                        t.append(np.eye(len(LABEL_MAP))[int(speeds[i+1])])\n",
    "                    # if the user is still holding the button ns after the rising edge, then it is passive data\n",
    "                    if (speeds[i+1:i+1+int(hold)+window] == speeds[i+1]).all() and \\\n",
    "                            len(data[i+1+int(hold):i+1+int(hold)+window]) == window:\n",
    "                        X.append(data[i+1+int(hold):i+1+int(hold)+window])\n",
    "                        t.append(np.eye(len(LABEL_MAP))[4])\n",
    "\n",
    "    return np.array(X).astype(np.float32).transpose(0, 2, 1), np.array(t).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d15164",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compile_direction_segments(files):\n",
    "    X = []\n",
    "    t = []\n",
    "\n",
    "    for file in files:\n",
    "        session = pd.read_csv(file, header=None).to_numpy()\n",
    "        \n",
    "        count = 0\n",
    "        timestamps = session[:, 0]\n",
    "        directions = session[:, 2]\n",
    "        data = session[:, 3:]\n",
    "        \n",
    "        plt.plot(timestamps, directions)\n",
    "        plt.title(\"Left right label\")\n",
    "        plt.show()\n",
    "        #plt.plot(timestamps, rolling_mode_smoothing(directions, SMOOTHING_WINDOW))\n",
    "        #plt.show()\n",
    "        \n",
    "        discon_segment = []\n",
    "        straight_segment = []\n",
    "        \n",
    "        for i in range(len(session)-1):\n",
    "            if timestamps[i+1] - timestamps[i] > 1000000000:\n",
    "                print(\"OSC delay too long\")\n",
    "                \n",
    "            # If the data was ambiguous, then skip\n",
    "            if directions[i] == 6:\n",
    "                continue\n",
    "                \n",
    "            # If the data is disconnected, collect in disconnected segments'\n",
    "            if directions[i] == 5:\n",
    "                discon_segment.append(data[i])\n",
    "                \n",
    "            # reaching max disconnected size save as segment\n",
    "            if len(discon_segment) == SEGMENT_WINDOW_SIZE:\n",
    "                X.append(discon_segment)\n",
    "                t.append(np.eye(len(label_map))[int(directions[i])])\n",
    "                discon_segment = []\n",
    "            \n",
    "            # Clear if reached end of disconnect\n",
    "            if directions[i] == 5 and directions[i+1] != 5:\n",
    "                discon_segment = []\n",
    "                \n",
    "            # If the direction is passive, collect as a segment\n",
    "            if directions[i] == 4:\n",
    "                straight_segment.append(data[i])\n",
    "            \n",
    "            if len(straight_segment) == SEGMENT_WINDOW_SIZE:\n",
    "                X.append(straight_segment)\n",
    "                t.append(np.eye(len(label_map))[int(directions[i])])\n",
    "                straight_segment = []\n",
    "                \n",
    "            if directions[i] == 4 and directions[i+1] != 4:\n",
    "                straight_segment = []\n",
    "            \n",
    "            if directions[i] != directions[i+1]:\n",
    "                # Save turn segments at the rising edge of every active region\n",
    "                if directions[i+1] in (2, 3):\n",
    "                    if (directions[i+1:i+ACTIVE_HOLD_PERIOD] == directions[i+1]).all() and \\\n",
    "                            len(data[i-int(SEGMENT_WINDOW_SIZE/2)+1:i+int(SEGMENT_WINDOW_SIZE/2)+1]) == SEGMENT_WINDOW_SIZE:\n",
    "                        X.append(data[i-int(SEGMENT_WINDOW_SIZE/2)+1:i+int(SEGMENT_WINDOW_SIZE/2)+1])\n",
    "                        t.append(np.eye(len(label_map))[int(directions[i+1])])\n",
    "                \n",
    "                \n",
    "    return np.array(X).astype(np.float32), np.array(t).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31439ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance categories\n",
    "def balance_categories(X, t):\n",
    "    max_per_label = min(t.sum(axis=0)[np.nonzero(t.sum(axis=0))])\n",
    "    counts = [max_per_label for i in range(len(t[0]))]\n",
    "    new_X = []\n",
    "    new_t = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if (np.array(new_t).sum(axis=0) + t[i] <= counts).all():\n",
    "            new_X.append(X[i])\n",
    "            new_t.append(t[i])\n",
    "            \n",
    "    return np.array(new_X), np.array(new_t)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56c9271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_model(shape, n_ff, n_sf, drop_rate, output_size):\n",
    "    \n",
    "    num_frames, num_channels, frame_size = shape\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(num_channels, frame_size, 1))\n",
    "\n",
    "    block1 = Conv2D(n_ff[0], (1, 16), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_1')(input_layer)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = DepthwiseConv2D((num_channels, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[0], activation = 'linear', \n",
    "                           depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_1')(block1)\n",
    "    block1 = BatchNormalization()(block1)\n",
    "    block1 = Activation('elu')(block1)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block2 = Conv2D(n_ff[1], (1, 32), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_2')(input_layer)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = DepthwiseConv2D((num_channels, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[1], activation = 'linear', \n",
    "                           depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_2')(block2)\n",
    "    block2 = BatchNormalization()(block2)\n",
    "    block2 = Activation('elu')(block2)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block3 = Conv2D(n_ff[2], (1, 64), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_3')(input_layer)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = DepthwiseConv2D((num_channels, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[2], activation = 'linear', \n",
    "                           depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_3')(block3)\n",
    "    block3 = BatchNormalization()(block3)\n",
    "    block3 = Activation('elu')(block3)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block4 = Conv2D(n_ff[3], (1, 128), use_bias = False, activation = 'linear', padding='same', name = 'Spectral_filter_4')(input_layer)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = DepthwiseConv2D((num_channels, 1), use_bias = False, padding='valid', depth_multiplier = n_sf[3], activation = 'linear', \n",
    "                           depthwise_constraint = tf.keras.constraints.MaxNorm(max_value=1), name = 'Spatial_filter_4')(block4)\n",
    "    block4 = BatchNormalization()(block4)\n",
    "    block4 = Activation('elu')(block4)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block = Concatenate(axis = -1)([block1, block2, block3, block4]) \n",
    "\n",
    "    #================================\n",
    "\n",
    "    block = AveragePooling2D((1, 4))(block)\n",
    "    block_in = Dropout(drop_rate)(block)\n",
    "\n",
    "    #================================\n",
    "\n",
    "    paddings = tf.constant([[0,0], [0,0], [3,0], [0,0]])\n",
    "    block = tf.pad(block_in, paddings, \"CONSTANT\")\n",
    "    block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 1))(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation('elu')(block)\n",
    "    block = Dropout(drop_rate)(block)\n",
    "    block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "    block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 1))(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation('elu')(block)\n",
    "    block = Dropout(drop_rate)(block)\n",
    "    block_out = Add()([block_in, block])\n",
    "\n",
    "\n",
    "    paddings = tf.constant([[0,0], [0,0], [6,0], [0,0]])\n",
    "    block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
    "    block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 2))(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation('elu')(block)\n",
    "    block = Dropout(drop_rate)(block)\n",
    "    block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "    block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 2))(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation('elu')(block)\n",
    "    block = Dropout(drop_rate)(block)\n",
    "    block_out = Add()([block_out, block])\n",
    "\n",
    "\n",
    "    paddings = tf.constant([[0,0], [0,0], [12,0], [0,0]])\n",
    "    block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
    "    block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 4))(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation('elu')(block)\n",
    "    block = Dropout(drop_rate)(block)\n",
    "    block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "    block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 4))(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation('elu')(block)\n",
    "    block = Dropout(drop_rate)(block)\n",
    "    block_out = Add()([block_out, block]) \n",
    "\n",
    "\n",
    "    paddings = tf.constant([[0,0], [0,0], [24,0], [0,0]])\n",
    "    block = tf.pad(block_out, paddings, \"CONSTANT\")\n",
    "    block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 8))(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation('elu')(block)\n",
    "    block = Dropout(drop_rate)(block)\n",
    "    block = tf.pad(block, paddings, \"CONSTANT\")\n",
    "    block = DepthwiseConv2D((1,4), padding=\"valid\", depth_multiplier=1, dilation_rate=(1, 8))(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation('elu')(block)\n",
    "    block = Dropout(drop_rate)(block)\n",
    "    block_out = Add()([block_out, block]) \n",
    "\n",
    "    #================================\n",
    "\n",
    "    block = block_out\n",
    "\n",
    "    #================================\n",
    "\n",
    "    block = Conv2D(28, (1,1))(block)\n",
    "    block = BatchNormalization()(block)\n",
    "    block = Activation('elu')(block)\n",
    "    block = AveragePooling2D((1,4), data_format='Channels_last')(block) #'Channels_last' As CPU will be used for inference\n",
    "    block = Dropout(drop_rate)(block) \n",
    "    embedded = Flatten()(block)\n",
    "\n",
    "    dense = Dense(128, activation='relu')(embedded)\n",
    "    dense = Dropout(drop_rate)(dense)\n",
    "    dense = Dense(64, activation='relu')(dense)\n",
    "    output = Dense(output_size, activation='softmax', kernel_constraint = max_norm(0.2))(dense)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1221c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, learning_rate, beta_1, beta_2):\n",
    "    adam = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, clipvalue=0.01)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "530f0f1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fit(model, X_train, t_train, X_val, t_val,\n",
    "        epochs, batch_size,\n",
    "        model_file, log_file):\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=model_file,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True)\n",
    "    results = model.fit(X_train, t_train, validation_data=(X_val, t_val), batch_size=batch_size, epochs=epochs, shuffle=True, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b1b1598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(attempt_num, iter_num,\n",
    "                     results, X_train, t_train, X_val, t_val, X_test, t_test,\n",
    "                     model_file, loss_graph, accuracy_graph, conf_matrix):\n",
    "    model = tf.keras.models.load_model(model_file)\n",
    "    test_results = model.evaluate(X_test, t_test, batch_size = 1)\n",
    "    train_results = model.evaluate(X_train, t_train, batch_size = 1)\n",
    "    val_results = model.evaluate(X_val, t_val, batch_size = 1)\n",
    "    \n",
    "    plt.plot(results.history['loss'])\n",
    "    plt.plot(results.history['val_loss'])\n",
    "    plt.plot([test_results[0] for i in range(len(results.history['loss']))])\n",
    "    plt.legend(('training loss', 'validation loss', 'test loss'))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Trial %s Attempt %s Interation %s Loss\" % (TRIAL_NUM, attempt_num, iter_num))\n",
    "    plt.grid()\n",
    "    plt.savefig(loss_graph)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(results.history['accuracy'])\n",
    "    plt.plot(results.history['val_accuracy'])\n",
    "    plt.plot([test_results[1] for i in range(len(results.history['accuracy']))])\n",
    "    plt.legend(('training accuracy', 'validation accuracy', 'test accuracy'))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Trial %s Attempt %s Interation %s Accuracy\" % (TRIAL_NUM, attempt_num, iter_num))\n",
    "    plt.grid()\n",
    "    plt.savefig(accuracy_graph)\n",
    "    plt.show()\n",
    "    \n",
    "    labels = ['BRAKE', 'ACCEL', 'PASSIVE', 'DISCONNECTED']\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    y_test = np.argmax(t_test, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    try:\n",
    "        plot = disp.plot()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    plt.savefig(conf_matrix)\n",
    "    plt.show()\n",
    "    \n",
    "    return train_results, val_results, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1300d5f",
   "metadata": {},
   "source": [
    "# Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "235abfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attempt(attempt_num,\n",
    "            xv_n_splits, max_iter,\n",
    "            hold_time, wait_time, window_time,\n",
    "            freq_filter_num, spatial_filter_num, drop_rate,\n",
    "            learning_rate, beta_1, beta_2,\n",
    "            epochs, batch_size\n",
    "            ):\n",
    "\n",
    "    # Make all directories and file names\n",
    "    attempt_dir = os.path.join(RESULTS_DIR, 'attempt%s' % attempt_num)\n",
    "    if not os.path.exists(attempt_dir):\n",
    "        os.makedirs(attempt_dir)\n",
    "    \n",
    "    interm_dir = os.path.join(attempt_dir, 'processed_data/')\n",
    "    if not os.path.exists(interm_dir):\n",
    "        os.makedirs(interm_dir)\n",
    "        \n",
    "    model_dir = os.path.join(attempt_dir, 'models/')\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "    accuracy_dir = os.path.join(attempt_dir, 'accuracy/')\n",
    "    if not os.path.exists(accuracy_dir):\n",
    "        os.makedirs(accuracy_dir)\n",
    "        \n",
    "    loss_dir = os.path.join(attempt_dir, 'loss/')\n",
    "    if not os.path.exists(loss_dir):\n",
    "        os.makedirs(loss_dir)\n",
    "        \n",
    "    conf_dir = os.path.join(attempt_dir, 'confusion/')\n",
    "    if not os.path.exists(conf_dir):\n",
    "        os.makedirs(conf_dir)\n",
    "\n",
    "    summary_file = os.path.join(attempt_dir, 'model_summary.txt')\n",
    "\n",
    "    log_file = os.path.join(attempt_dir, 'training.log')\n",
    "    \n",
    "    stdout = sys.stdout\n",
    "    f = open(log_file, 'w')\n",
    "    sys.stdout = f\n",
    "    \n",
    "    # Get files\n",
    "    files = shuffle(glob.glob(os.path.join(DATA_PATH, 'ridvan*.csv')))\n",
    "    \n",
    "    file_data = []\n",
    "    \n",
    "    # compile files\n",
    "    for file in files:\n",
    "        print('Converting file %s' % file)\n",
    "        X_file = os.path.join(interm_dir, '%s_X.npy' % os.path.splitext(os.path.basename(file))[0])\n",
    "        t_file = os.path.join(interm_dir, '%s_t.npy' % os.path.splitext(os.path.basename(file))[0])\n",
    "        if os.path.exists(X_file) and os.path.exists(t_file):\n",
    "            X = np.load(X_file)\n",
    "            t = np.load(t_file)\n",
    "        else:\n",
    "            X, t = compile_speed_segments([file], hold=hold_time, wait=wait_time, window=window_time)\n",
    "            X, t = balance_categories(X, t)\n",
    "            np.save(X_file, X)\n",
    "            np.save(t_file, t)\n",
    "        file_data.append((X, t))\n",
    "        \n",
    "    print('Total of %s datasets' % len(file_data))\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test, t_test = file_data.pop()\n",
    "    print(\"Test Sizes: \")\n",
    "    print(X_test.shape)\n",
    "    print(t_test.shape)\n",
    "    \n",
    "    xv_results = defaultdict(list)\n",
    "    \n",
    "    # iterate through cross validation steps\n",
    "    iteration = 0\n",
    "    for train_index, val_index in KFold(xv_n_splits).split(file_data):\n",
    "        if iteration == max_iter:\n",
    "            break\n",
    "        iteration += 1\n",
    "        model_file = os.path.join(model_dir, '%s_model.h5' % iteration)\n",
    "        accuracy_graph = os.path.join(accuracy_dir, '%s_accuracy.png' % iteration)\n",
    "        loss_graph = os.path.join(loss_dir, '%s_loss.png' % iteration)\n",
    "        conf_matrix = os.path.join(conf_dir, '%s_confusion.png' % iteration)\n",
    "        \n",
    "        # split using cross validation steps\n",
    "        X_train = np.concatenate(np.array(file_data)[train_index][:, 0])\n",
    "        t_train = np.concatenate(np.array(file_data)[train_index][:, 1])\n",
    "        X_val = np.concatenate(np.array(file_data)[val_index][:, 0])\n",
    "        t_val = np.concatenate(np.array(file_data)[val_index][:, 1])\n",
    "        \n",
    "        print('Iteration %s' % iteration)\n",
    "        print(\"Train sizes: \")\n",
    "        print(X_train.shape)\n",
    "        print(t_train.shape)\n",
    "        print(\"Val sizes: \")\n",
    "        print(X_val.shape)\n",
    "        print(t_val.shape)\n",
    "        \n",
    "        # Load model\n",
    "        model = create_model(X_test.shape, freq_filter_num, spatial_filter_num, drop_rate, t_test.shape[-1])\n",
    "\n",
    "        # Print model summary and save\n",
    "        with open(summary_file, 'w') as fh:\n",
    "            # Pass the file handle in as a lambda function to make it callable\n",
    "            model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "            fh.close()\n",
    "            \n",
    "        compile_model(model, learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2)\n",
    "            \n",
    "        results = fit(model, X_train, t_train, X_val, t_val, epochs, batch_size, model_file, log_file)\n",
    "        \n",
    "        train_results, val_results, test_results = evaluate_results(attempt_num, iteration,\n",
    "                                        results, X_train, t_train, X_val, t_val, X_test, t_test,\n",
    "                                        model_file, loss_graph, accuracy_graph, conf_matrix)\n",
    "        \n",
    "        xv_results['train_acc'].append(train_results[1])\n",
    "        xv_results['train_loss'].append(train_results[0])\n",
    "        xv_results['val_acc'].append(val_results[1])\n",
    "        xv_results['val_loss'].append(val_results[0])\n",
    "        xv_results['test_acc'].append(test_results[1])\n",
    "        xv_results['test_loss'].append(test_results[0])\n",
    "        \n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    \n",
    "    sys.stdout = stdout\n",
    "    f.close()\n",
    "        \n",
    "    return xv_results\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9f5cd4",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26915840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 09:41:48.910628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2023-04-10 09:41:49.011951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:21:00.0 name: Tesla K20Xm computeCapability: 3.5\n",
      "coreClock: 0.732GHz coreCount: 14 deviceMemorySize: 5.57GiB deviceMemoryBandwidth: 232.46GiB/s\n",
      "2023-04-10 09:41:49.012345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-10 09:41:49.014607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-10 09:41:49.016403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-10 09:41:49.016828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-10 09:41:49.019080: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-10 09:41:49.020229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-10 09:41:49.025647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-10 09:41:49.027141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2023-04-10 09:41:49.046180: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2294455000 Hz\n",
      "2023-04-10 09:41:49.048539: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f4fe4000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-10 09:41:49.048579: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-04-10 09:41:49.174031: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5688130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-10 09:41:49.174101: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K20Xm, Compute Capability 3.5\n",
      "2023-04-10 09:41:49.175602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:21:00.0 name: Tesla K20Xm computeCapability: 3.5\n",
      "coreClock: 0.732GHz coreCount: 14 deviceMemorySize: 5.57GiB deviceMemoryBandwidth: 232.46GiB/s\n",
      "2023-04-10 09:41:49.175723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-10 09:41:49.175790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-10 09:41:49.175854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-10 09:41:49.175918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-10 09:41:49.175982: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-10 09:41:49.176045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-10 09:41:49.176111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-10 09:41:49.178189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n",
      "2023-04-10 09:41:49.178317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-10 09:41:49.179625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-04-10 09:41:49.179660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n",
      "2023-04-10 09:41:49.179678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n",
      "2023-04-10 09:41:49.181914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5300 MB memory) -> physical GPU (device: 0, name: Tesla K20Xm, pci bus id: 0000:21:00.0, compute capability: 3.5)\n",
      "2023-04-10 09:41:55.832282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-10 09:41:56.003136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    }
   ],
   "source": [
    "window_times = [100, 200, 300, 400]\n",
    "wait_times = [100]\n",
    "hold_times = [50, 150, 250]\n",
    "learning_rates = [0.01]\n",
    "\n",
    "results = defaultdict(list)\n",
    "attempt_num = 1\n",
    "\n",
    "for window_time, wait_time, hold_time, learning_rate in product(window_times, wait_times, hold_times, learning_rates):\n",
    "    max_iter = 3\n",
    "    attempt_results = attempt(attempt_num=attempt_num,\n",
    "                      xv_n_splits=6, max_iter=max_iter,\n",
    "                      hold_time=hold_time, wait_time=wait_time, window_time=window_time,\n",
    "                      freq_filter_num=[2,4,8,16], spatial_filter_num=[1,1,1,1], drop_rate=0.2,\n",
    "                      learning_rate=learning_rate, beta_1=0.9, beta_2=0.999,\n",
    "                      epochs=1500, batch_size=32\n",
    "                      )\n",
    "\n",
    "    results['attempt'].append(attempt_num)\n",
    "    results['window'].append(window_time)\n",
    "    results['wait'].append(wait_time)\n",
    "    results['hold'].append(hold_time)\n",
    "    results['learning_rate'].append(learning_rate)\n",
    "    results['train_acc'].append(sum(attempt_results['train_acc'])/max_iter)\n",
    "    results['train_loss'].append(sum(attempt_results['train_loss'])/max_iter)\n",
    "    results['val_acc'].append(sum(attempt_results['val_acc'])/max_iter)\n",
    "    results['val_loss'].append(sum(attempt_results['val_loss'])/max_iter)\n",
    "    results['test_acc'].append(sum(attempt_results['test_acc'])/max_iter)\n",
    "    results['test_loss'].append(sum(attempt_results['test_loss'])/max_iter)\n",
    "    \n",
    "    attempt_num += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc_fig = os.path.join(RESULTS_DIR, 'experiment%s_test_acc.png' % EXPER_NUM)\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.plot_trisurf(results['window'], results['learning_rate'], results['test_acc'], cmap ='viridis')\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('hold')\n",
    "plt.title('Test Accuracy')\n",
    "\n",
    "plt.savefig(test_acc_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fc1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_fig = os.path.join(RESULTS_DIR, 'experiment%s_test_loss.png' % EXPER_NUM)\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.plot_trisurf(results['window'], results['learning_rate'], results['test_loss'], cmap ='viridis')\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('hold')\n",
    "plt.title('Test Loss')\n",
    "\n",
    "plt.savefig(test_loss_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe44e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_fig = os.path.join(RESULTS_DIR, 'experiment%s_train_acc.png' % EXPER_NUM)\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.plot_trisurf(results['window'], results['learning_rate'], results['train_acc'], cmap ='viridis')\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('hold')\n",
    "plt.title('Train Accuracy')\n",
    "\n",
    "plt.savefig(train_acc_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c817ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_fig = os.path.join(RESULTS_DIR, 'experiment%s_train_loss.png' % EXPER_NUM)\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.plot_trisurf(results['window'], results['learning_rate'], results['train_loss'], cmap ='viridis')\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('hold')\n",
    "plt.title('Train Loss')\n",
    "\n",
    "plt.savefig(train_loss_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b899e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_fig = os.path.join(RESULTS_DIR, 'experiment%s_val_acc.png' % EXPER_NUM)\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.plot_trisurf(results['window'], results['learning_rate'], results['val_acc'], cmap ='viridis')\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('hold')\n",
    "plt.title('Validation Accuracy')\n",
    "\n",
    "plt.savefig(val_acc_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3038ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_fig = os.path.join(RESULTS_DIR, 'experiment%s_val_loss.png' % EXPER_NUM)\n",
    "ax = plt.axes(projection ='3d')\n",
    "ax.plot_trisurf(results['window'], results['learning_rate'], results['val_loss'], cmap ='viridis')\n",
    "plt.xlabel('window')\n",
    "plt.ylabel('hold')\n",
    "plt.title('Validation Loss')\n",
    "\n",
    "plt.savefig(val_loss_fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
