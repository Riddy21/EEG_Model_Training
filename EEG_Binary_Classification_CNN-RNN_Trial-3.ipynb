{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31de1cd",
   "metadata": {},
   "source": [
    "# EEG Wheelchair Binary Classification ML Model\n",
    "## Using 1D Convolutional Networks and Recurrent Networks\n",
    "This is the first experiment with trying to convert 4 channel EEG brain wave data into a binary classification of stop and go for the EEG wheelchair control system.\n",
    "\n",
    "## Overview\n",
    "The following notebook will follow these next steps:\n",
    "1. Data Cleaning\n",
    "    - Get rid of inconsistent samples or bad samples\n",
    "2. Data Preprocessing\n",
    "    - Convert to wavelet transforms and take signal squeezed signals\n",
    "3. Data Filtering\n",
    "    - Filter out irrelevant frequencies and disconnections\n",
    "4. Build Model\n",
    "    - Form CNN-RNN network for prediction modelling\n",
    "5. Model Training\n",
    "    - Train model on training set of EEG Samples\n",
    "6. Hyperparameter Optimization\n",
    "    - Optimize model hyperparameters by cross validation \n",
    "7. Model Validation\n",
    "    - Validate model on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e41fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import random\n",
    "\n",
    "# Data management\n",
    "import pandas as pd\n",
    "\n",
    "# Data processing\n",
    "from ssqueezepy import ssq_cwt\n",
    "\n",
    "# Model training\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Input, TimeDistributed\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, AveragePooling1D\n",
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5880c54",
   "metadata": {},
   "source": [
    "## Experiment Information\n",
    "Record the experiment information in these fields to be saved with the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "178584e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_num = 3\n",
    "attempt_num = 2\n",
    "input_dim = 195\n",
    "output_dim = 3\n",
    "attempt_description = \\\n",
    "\"\"\"\n",
    "## 2nd attempt - More data\n",
    "\n",
    "### Changes made\n",
    "- Trying the model with different dataset found online\n",
    "    - https://www.nature.com/articles/sdata2018211\n",
    "- Using the LR hand paradigm\n",
    "- Baseline:\n",
    "    - Original model, 34 samples per prediction\n",
    "- Adding more epochs\n",
    "\n",
    "### Expected results\n",
    "- Takes longer to train, need more epochs\n",
    "- Should overfit a little later\n",
    "- Decrease in overall accuracy probably\n",
    "\n",
    "### Issues noticed\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878854c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories to save data\n",
    "model_dir = './models/trial%s' % trial_num\n",
    "results_dir = './results/trial%s/attempt%s' % (trial_num, attempt_num)\n",
    "interm_dir = './processed_data/trial%s/attempt%s' % (trial_num, attempt_num)\n",
    "\n",
    "model_file = os.path.join(model_dir, 'attempt%s_%s.h5' % (attempt_num, input_dim))\n",
    "description_file = os.path.join(results_dir, 'attempt%s_readme.md' % attempt_num)\n",
    "accuracy_graph = os.path.join(results_dir, 'attempt%s_accuracy.png' % attempt_num)\n",
    "loss_graph = os.path.join(results_dir, 'attempt%s_loss.png' % attempt_num)\n",
    "conf_matrix = os.path.join(results_dir, 'attempt%s_confusion.png' % attempt_num)\n",
    "log_file = os.path.join(results_dir, 'attempt%s.log' % attempt_num)\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "if not os.path.exists(interm_dir):\n",
    "    os.makedirs(interm_dir)\n",
    "    \n",
    "# Save description\n",
    "with open(description_file, 'w') as f:\n",
    "    f.write(attempt_description)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb0e2b",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aae3780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(621892,) (621892, 22)\n"
     ]
    }
   ],
   "source": [
    "#### FOR RUNNING OPENBCI DATA ####\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('./datasets/openbci_dataset/CLA-SubjectJ-170508-3St-LRHand-Inter.mat')\n",
    "data = mat['o']['data'][0][0]\n",
    "labels = mat['o']['marker'][0][0].flatten()\n",
    "print(labels.shape, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d2b86",
   "metadata": {},
   "source": [
    "### Try test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad4b33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Try test file\n",
    "\n",
    "# Convert to numpy arrays\n",
    "tp9, af7, af8, tp10 = data[28500:28500+170].T[:4]\n",
    "\n",
    "# Get the frequency of the samples \n",
    "Twtp9, Wtp9, *_ = ssq_cwt(tp9, fs=200)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "fig.add_subplot(4, 1, 1)\n",
    "plt.plot(data[28500:28500+170])\n",
    "fig.add_subplot(4, 1, 3)\n",
    "plt.imshow(np.abs(Twtp9), aspect='auto', cmap='turbo')\n",
    "fig.add_subplot(4, 1, 2)\n",
    "plt.imshow(np.abs(Wtp9), aspect='auto', cmap='turbo')\n",
    "fig.add_subplot(4, 1, 4)\n",
    "plt.plot(labels[28500:28500+170])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0048c9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:\n",
      "    There are 289 segments with LEFT label of shape (4, 170)\n",
      "    There are 327 segments with RIGHT label of shape (4, 170)\n",
      "    There are 283 segments with PASSIVE label of shape (4, 170)\n",
      "Training: \n",
      "    There are 226 segments with LEFT label of shape (226, 4, 170)\n",
      "    There are 226 segments with RIGHT label of shape (226, 4, 170)\n",
      "    There are 226 segments with PASSIVE label of shape (226, 4, 170)\n",
      "Testing: \n",
      "    There are 28 segments with LEFT label of shape (28, 4, 170)\n",
      "    There are 28 segments with RIGHT label of shape (28, 4, 170)\n",
      "    There are 28 segments with PASSIVE label of shape (28, 4, 170)\n",
      "Validation: \n",
      "    There are 29 segments with LEFT label of shape (29, 4, 170)\n",
      "    There are 29 segments with RIGHT label of shape (29, 4, 170)\n",
      "    There are 29 segments with PASSIVE label of shape (29, 4, 170)\n"
     ]
    }
   ],
   "source": [
    "##### For our data\n",
    "segments = defaultdict(list)\n",
    "\n",
    "# Number of samples per file\n",
    "SEGMENT_SIZE = 170\n",
    "\n",
    "# Number of channels per file\n",
    "CHANNELS = [1, 2, 17, 18]\n",
    "#CHANNELS = [i for i in range(1,23)]\n",
    "\n",
    "# label_map = {1: \"LEFT\",\n",
    "#              2: \"RIGHT\",\n",
    "#              3: \"PASSIVE\",\n",
    "#              0: \"REMOVED\"}\n",
    "\n",
    "# Only use the left right passive signals\n",
    "label_map = {1: \"LEFT\",\n",
    "             2: \"RIGHT\",\n",
    "             3: \"PASSIVE\"}\n",
    "\n",
    "# Compile each 1s session into file\n",
    "segment = []\n",
    "for i in range(len(labels)):\n",
    "    try:\n",
    "        label = label_map[labels[i]]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    # Append the previous file and open new file\n",
    "    if i+1 == len(labels) or labels[i] != labels[i+1] or len(segment) == SEGMENT_SIZE:\n",
    "        # Split any that are large then FILE_SIZE and discard any that is smaller\n",
    "        if len(segment) == SEGMENT_SIZE:\n",
    "            segments[label].append(np.array(segment).T)\n",
    "        segment = []\n",
    "        \n",
    "    segment.append(np.take(data[i], [chan - 1 for chan in CHANNELS]))\n",
    "    \n",
    "    \n",
    "print(\"Total:\")\n",
    "for val in label_map.values():\n",
    "    print('    There are %d segments with %s label of shape %s' % (len(segments[val]), val, segments[val][0].shape))\n",
    "    \n",
    "max_num_segments = min([len(segments[key]) for key in label_map.values()])\n",
    "\n",
    "train_segments = defaultdict(list)\n",
    "test_segments = defaultdict(list)\n",
    "val_segments = defaultdict(list)\n",
    "\n",
    "#             [train, test, vali]\n",
    "split_ratio = [0.8, 0.1, 0.1]\n",
    "\n",
    "# Split the data to training test and val\n",
    "for key in label_map.values():\n",
    "    train_segments[key], test_segments[key], val_segments[key] = \\\n",
    "            np.split(shuffle(segments[key])[:max_num_segments], # Shuffle the data\n",
    "                     [int(sum(split_ratio[:i+1])*max_num_segments) for i in range(len(split_ratio)-1)])\n",
    "\n",
    "print(\"Training: \")\n",
    "for val in label_map.values():\n",
    "    print('    There are %d segments with %s label of shape %s' % (len(train_segments[val]), val, train_segments[val].shape))\n",
    "    \n",
    "print(\"Testing: \")\n",
    "for val in label_map.values():\n",
    "    print('    There are %d segments with %s label of shape %s' % (len(test_segments[val]), val, test_segments[val].shape))\n",
    "    \n",
    "print(\"Validation: \")\n",
    "for val in label_map.values():\n",
    "    print('    There are %d segments with %s label of shape %s' % (len(val_segments[val]), val, val_segments[val].shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac7e119",
   "metadata": {},
   "source": [
    "## Convert to wavelet transform signal squeezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbab74cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 200\n",
    "def ts_to_ssq(timeseries):\n",
    "    \"\"\"\n",
    "    Converts x channel time series data to ssq wavelet transform\n",
    "    \"\"\"\n",
    "    ssqcwt = []\n",
    "    for channel in timeseries:\n",
    "        out, _, *_ = ssq_cwt(channel, fs=SAMPLE_RATE)\n",
    "        ssqcwt.append(out)\n",
    "    \n",
    "    return np.array(ssqcwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8fc6b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sample_size = SEGMENT_SIZE\n",
    "buffer_sample_size = SEGMENT_SIZE # 0.85s\n",
    "training_sample_size = 34 # 0.175s\n",
    "# training_sample_size = SEGMENT_SIZE\n",
    "\n",
    "key_dict = {\"LEFT\":(1.0, 0.0, 0.0),\n",
    "            \"RIGHT\":(0.0, 1.0, 0.0),\n",
    "            \"PASSIVE\": (0.0, 0.0, 1.0)}\n",
    "\n",
    "\n",
    "def compile_segments(segments, seg_type):\n",
    "    X = []\n",
    "    t = []\n",
    "    \n",
    "    num_segments = sum([len(x) for x in segments.values()])\n",
    "    \n",
    "    for key, data in segments.items():\n",
    "        for recording in data:\n",
    "            # if segment is smaller than sample size\n",
    "            if len(recording[0]) < original_sample_size:\n",
    "                continue\n",
    "\n",
    "            # Split file into buffer size samples\n",
    "            time_samples = np.split(recording,\n",
    "                                    int(original_sample_size/buffer_sample_size), axis=1)\n",
    "\n",
    "            for time_sample in time_samples:\n",
    "                # Convert to numpy arrays\n",
    "                ssq_data = ts_to_ssq(time_sample)\n",
    "\n",
    "                # Take the magnitude of the wavelet transform only\n",
    "                ssq_data = np.abs(ssq_data)\n",
    "\n",
    "                # Must transpose data to (sample, height, channel) for training\n",
    "                ssq_data = ssq_data.transpose(2,1,0)\n",
    "\n",
    "                # Split sample into many blocks\n",
    "                wavelet_samples = np.split(ssq_data, int(buffer_sample_size/training_sample_size))\n",
    "\n",
    "                X.extend(wavelet_samples)\n",
    "                t.extend([key_dict[key] for i in range(len(wavelet_samples))])\n",
    "                \n",
    "            num_segments -= 1\n",
    "            \n",
    "            print(\"%d %s segments left to convert \" % (num_segments, seg_type), end=\"\\r\")\n",
    "                \n",
    "    return np.array(X), np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27cea15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 train segments left to convert  \r"
     ]
    }
   ],
   "source": [
    "# Compile all files into train data\n",
    "X_train, t_train = compile_segments(train_segments, 'train')\n",
    "\n",
    "X_test, t_test = compile_segments(test_segments, 'test')\n",
    "\n",
    "X_val, t_val = compile_segments(val_segments, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f695b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Training data\")\n",
    "print(X_train.shape)\n",
    "print(t_train.shape)\n",
    "print(\"Testing data\")\n",
    "print(X_test.shape)\n",
    "print(t_test.shape)\n",
    "print(\"Validation data\")\n",
    "print(X_val.shape)\n",
    "print(t_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef006cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ssq data\n",
    "np.save(os.path.join(interm_dir, 'x_train.npy'), X_train)\n",
    "np.save(os.path.join(interm_dir, 't_train.npy'), t_train)\n",
    "np.save(os.path.join(interm_dir, 'x_test.npy'), X_test)\n",
    "np.save(os.path.join(interm_dir, 't_test.npy'), t_test)\n",
    "np.save(os.path.join(interm_dir, 'x_val.npy'), X_val)\n",
    "np.save(os.path.join(interm_dir, 't_val.npy'), t_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a794b1a",
   "metadata": {},
   "source": [
    "## *Shortcut: Load Preprocessed Saved Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ssq data\n",
    "X_train = np.load(os.path.join(interm_dir, 'x_train.npy'))\n",
    "t_train = np.load(os.path.join(interm_dir, 't_train.npy'))\n",
    "X_test = np.load(os.path.join(interm_dir, 'x_test.npy'))\n",
    "t_test = np.load(os.path.join(interm_dir, 't_test.npy'))\n",
    "np.load(os.path.join(interm_dir, 'x_val.npy'))\n",
    "np.load(os.path.join(interm_dir, 't_val.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b83e2",
   "metadata": {},
   "source": [
    "## Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "X_train, t_train = shuffle(X_train, t_train)\n",
    "X_test, t_test = shuffle(X_test, t_test)\n",
    "X_val, t_val = shuffle(X_val, t_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ffeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [0, 0, 0]\n",
    "for data in t_train:\n",
    "    for i in range(3):\n",
    "        if data[i] == 1:\n",
    "            counts[i] += 1\n",
    "  \n",
    "print(\"Balance of categories\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6645adbf",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e312d4df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Declare a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Declare input shape of\n",
    "model.add(Input(shape=X_train[0].shape)) # shape = samples, height, channels\n",
    "\n",
    "# First add Convolution and Pooling layers\n",
    "model.add(TimeDistributed(Conv1D(253, (3))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "\n",
    "model.add(TimeDistributed(Conv1D(253, (2))))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=(2))))\n",
    "\n",
    "# Flatten to prepare for Recurrent network\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# Recurrent layers connect to each of the time steps\n",
    "model.add(GRU(128, activation='relu', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu')\n",
    "         )\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Output is 3, REST, GO, STOP likelihood\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f605c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544ba26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stdout = sys.stdout\n",
    "f = open(log_file, 'w')\n",
    "sys.stdout = f\n",
    "results = model.fit(X_train, t_train, validation_data=(X_val, t_val), batch_size=5, epochs=30)\n",
    "sys.stdout = stdout\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac6764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the model\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa9522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_results = model.evaluate(X_test, t_test, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb4994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e6666c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918cf37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.plot([test_results[0] for i in range(len(results.history['loss']))])\n",
    "plt.legend(('training loss', 'validation loss', 'test loss'))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Trial %s Attempt %s Loss\" % (trial_num, attempt_num))\n",
    "plt.grid()\n",
    "plt.savefig(loss_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bdf3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(results.history['accuracy'])\n",
    "plt.plot(results.history['val_accuracy'])\n",
    "plt.plot([test_results[1] for i in range(len(results.history['accuracy']))])\n",
    "plt.legend(('training accuracy', 'validation accuracy', 'test accuracy'))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Trial %s Attempt %s Accuracy\" % (trial_num, attempt_num))\n",
    "plt.grid()\n",
    "plt.savefig(accuracy_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ceb807",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "y_test = np.argmax(t_test, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='pred')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=key_dict.keys())\n",
    "plot = disp.plot()\n",
    "disp.figure_.savefig(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7cac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
